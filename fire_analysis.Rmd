---
title: "Thomas Fire Sentiment Analysis"
author: "Joe Desacaro, Connor Flynn, Grace Lewin, Shale Hunter, Steven Cognac"
date: '`r Sys.Date()`'
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

#### Thomas Fire Sentiment Analysis - Twitter

```{r packages, results='hide', message=FALSE, warning=FALSE}
library(quanteda)
#devtools::install_github("quanteda/quanteda.sentiment") #not available currently through CRAN
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud) #visualization of common words in the data set
library(reshape2)
library(here)
library(wesanderson)
```

Read in the data

```{r}
all_tweet_files <- list.files(path = here("data/"),
                              pattern = "*.csv")

```


```{r}
temp_df <- data.frame() # make empty df to put new data into

for (i in 1:length(all_tweet_files)){
  current_file <- read_csv(file = here("data",all_tweet_files[i]), # read current csv
                           skip = 6) # skip first 6 lines of csv because they are not helpful
  temp_df <- rbind(temp_df, # bind to df
                   current_file)
  print(paste("Done with",i))
}
```


```{r tweet_data}

data <- temp_df[,c(4,5)] # Extract Date and Title fields from temp_df

tweets <- tibble(text = data$Title, # make tweet text column
                 id = seq(1:length(data$Title)), # make id sequence
                 date = as.Date(data$Date,'%m/%d/%y')) %>%  # make date column
  mutate(text = str_replace(string = text,
                            pattern = "http.*[:space:]",
                            replacement = ""),
         text = str_replace(string = text,
                            pattern = "http.*$",
                            replacement = ""),
         text = str_replace(string = text,
                            pattern = "@.*[:space:]",
                            replacement = ""),
         text = str_replace(string = text,
                            pattern = "@.*$",
                            replacement = ""),
         text = str_replace_all(string = text,
                               pattern = "rt",
                               replacement = ""),
         text = str_to_lower(text))

tweets$text <- iconv(tweets$text, 
                     "latin1", 
                     "ASCII", 
                     sub="")

#simple plot of tweets per day
tweets %>%
  count(date) %>% 
  ggplot(aes(x = date, y = n)) +
  geom_line() +
  labs(title = "Number of Tweets Per Day",
       x = "Count",
       y = "Day") 

```

```{r clean_tweets}

#let's clean up the URLs from the tweets
# tweets$text <- gsub("http[^[:space:]]*", "",tweets$text) # pull out https and urls and convert to blank

#load sentiment lexicons
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')

#tokenize tweets to individual words
words <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  left_join(nrc_sent, by = "word") %>%
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment") %>% 
  filter(word != "rt") # remove rt as a word

```

```{r sentiment_calculations}
#take average sentiment score by tweet
tweets_sent <- tweets %>%
  left_join(
    words %>%
      group_by(id) %>%
      summarize(
        sent_score = mean(sent_score, na.rm = T)),
    by = "id")

neutral <- length(which(tweets_sent$sent_score == 0))
positive <- length(which(tweets_sent$sent_score > 0))
negative <- length(which(tweets_sent$sent_score < 0))

Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
  geom_bar(stat = "identity", aes(fill = Sentiment))+
  scale_fill_manual("legend", values = c("Positive" = "green", "Neutral" = "black", "Negative" = "red"))+
  ggtitle("Barplot of Sentiment in Santa Barbara tweets")
```

```{r plot_sentiment_by_day}
# tally sentiment score per day
daily_sent <- tweets_sent %>%
  group_by(date) %>%
  summarize(sent_score = mean(sent_score, na.rm = T))

daily_sent %>%
  ggplot( aes(x = date, y = sent_score)) +
  geom_line() +
    labs(x = "Date",
    y = "Avg Sentiment Score",
    title = "Daily Tweet Sentiment",
    subtitle = "Thomas Fire Tweets") 

```

```{r wordcloud_comp}
words %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("red4", "darkgreen"),
                   max.words = 100)
```

#### The quanteda package

quanteda is a package (actually a family of packages) full of tools for conducting text analysis. quanteda.sentiment (not yet on CRAN, download from github) is the quanteda modular package for conducting sentiment analysis.

quanteda has its own built in functions for cleaning text data. Let's take a look at some. First we have to clean the messy tweet data:

```{r create_corpus, echo=FALSE}
corpus <- corpus(data$Title) #enter quanteda
summary(corpus)
```

```{r quanteda_cleaning}
tokens <- tokens(corpus) #tokenize the text so each doc (page, in this case) is a list of tokens (words)

#examine the uncleaned version
tokens

#clean it up
tokens <- tokens(tokens, remove_punct = TRUE,
                      remove_numbers = TRUE)

tokens <- tokens_select(tokens, stopwords('english'),selection='remove') #stopwords lexicon built in to quanteda

#tokens <- tokens_wordstem(tokens) #stem words down to their base form for comparisons across tense and quantity
# since we are doing visual analysis we are leaving this out but if we were doing rigorous analysis we would want to do it.

tokens <- tokens_tolower(tokens)

```

I don't think we need this
```{r explore_hashtags}
# hash_tweets <- tokens(corpus, remove_punct = TRUE) %>% 
#                tokens_keep(pattern = "#*")
# 
# dfm_hash<- dfm(hash_tweets)
# 
# tstat_freq <- textstat_frequency(dfm_hash, n = 100)
# head(tstat_freq, 10)
# 
# #tidytext gives us tools to convert to tidy from non-tidy formats
# hash_tib<- tidy(dfm_hash)
# 
# hash_tib %>%
#    count(term) %>%
#    with(wordcloud(term, n, max.words = 100))


```


Create the sparse matrix representation known as the document-feature matrix. quanteda's textstat_polarity function has multiple ways to combine polarity to a single score. The sent_logit value to fun argument is the log of (pos/neg) counts.

```{r}

dfm <- dfm(tokens)

topfeatures(dfm, 12)

dfm.sentiment <- dfm_lookup(dfm, dictionary = data_dictionary_LSD2015)

head(textstat_polarity(tokens, data_dictionary_LSD2015, fun = sent_logit))


```


```{r}
ten_words <- words %>% 
  group_by(date, word) %>% 
  summarise(count = n()) %>% 
  group_by(date) %>% 
  slice_max(count, 
            n = 10,
            with_ties = FALSE) 


ten_words %>%
  ggplot(aes(x = count,
             y = word)) +
  geom_col() +
  facet_wrap(~date, scales = "free") +
  guides(fill = "none") +
  labs(x = "Number of Times Word Occurs",
       y = "",
       title = "Top 10 Words per Day")
```

### Make word groups

```{r}
words_before <- words %>% 
  filter(date < "2017-12-05")

words_during <- words %>% 
  filter(date >= "2017-12-05")
```

### Make some charts of before and during
```{r}
before_counts <- words_before %>% 
  group_by(sentiment) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>% 
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  filter(!is.na(sentiment)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL,
       title = "Most Used Words for Each Sentiment Before Fire")

before_counts
```

```{r}
during_counts <- words_during %>% 
  group_by(sentiment) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup() %>% 
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  filter(!is.na(sentiment)) %>% 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL,
       title = "Most Used Words for Each Sentiment During Fire")

during_counts
```

## Make plot that shows sentiment over time

```{r}
sent_word_count <- words %>%
  group_by(date, sentiment) %>%
  count(sentiment) %>%
  ungroup() %>%
  filter(!is.na(sentiment)) %>% 
  group_by(date) %>%
  mutate(n_max = sum(n),
         percent = round((n / n_max) * 100, 2))

ggplot(data = sent_word_count) +
  geom_line(aes(x = date, y = percent, color = sentiment)) +
  labs(title = "Sentiment Before and During Fire",
       y = "Percent",
       x = "Date") +
  scale_color_manual(values = c("#A6CEE3",
                                "#1F78B4",
                                "#B2DF8A",
                                "#33A02C",
                                "#FB9A99",
                                "#E31A1C",
                                "#FDBF6F",
                                "#FF7F00",
                                "#CAB2D6",
                                "#6A3D9A")) +
  theme_minimal() + 
  geom_vline(xintercept = as.Date("2017-12-05"),
             linetype = "dashed",
             size = 1)
```

