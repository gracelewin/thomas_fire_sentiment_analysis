---
title: "Topic 4: Sentiment Analysis II"
---

This .Rmd available here: <https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/topic_4.Rmd>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#### IPCC Report Twitter

```{r packages, results='hide', message=FALSE, warning=FALSE}
library(quanteda)
#devtools::install_github("quanteda/quanteda.sentiment") #not available currently through CRAN
library(quanteda.sentiment)
library(quanteda.textstats)
library(tidyverse)
library(tidytext)
library(lubridate)
library(wordcloud) #visualization of common words in the data set
library(reshape2)
library(here)
```

Read in the data

```{r}
tweet_test <- read_csv(file = here("data/d01_am.csv"),
                       delim = ",")

all_tweet_files <- list.files(path = here("data/"),
                              pattern = "*.csv")

```


```{r}
temp_df <- data.frame()

for (i in 1:length(all_tweet_files)){
  current_file <- read_csv(file = here("data",all_tweet_files[i]),
                           skip = 6)
  temp_df <- rbind(temp_df,
                   current_file)
  print(paste("Done with",i))
}
```


```{r tweet_data}

data <- temp_df[,c(4,5)] # Extract Date and Title fields

tweets <- tibble(text = data$Title,
                 id = seq(1:length(data$Title)),
                 date = as.Date(data$Date,'%m/%d/%y'))


#simple plot of tweets per day
tweets %>%
  count(date) %>%
  ggplot(aes(x = date, y = n))+
  geom_line()

```

```{r cleaning_tweets}

#let's clean up the URLs from the tweets
tweets$text <- gsub("http[^[:space:]]*", "",tweets$text) # pull out https and urls and convert to blank
tweets$text <- str_to_lower(tweets$text)

#load sentiment lexicons
bing_sent <- get_sentiments('bing')
nrc_sent <- get_sentiments('nrc')

#tokenize tweets to individual words
words <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  left_join(bing_sent, by = "word") %>%
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")
```

```{r sentiment_calculations}
#take average sentiment score by tweet
tweets_sent <- tweets %>%
  left_join(
    words %>%
      group_by(id) %>%
      summarize(
        sent_score = mean(sent_score, na.rm = T)),
    by = "id")

neutral <- length(which(tweets_sent$sent_score == 0))
positive <- length(which(tweets_sent$sent_score > 0))
negative <- length(which(tweets_sent$sent_score < 0))

Sentiment <- c("Positive","Neutral","Negative")
Count <- c(positive,neutral,negative)
output <- data.frame(Sentiment,Count)
output$Sentiment<-factor(output$Sentiment,levels=Sentiment)
ggplot(output, aes(x=Sentiment,y=Count))+
  geom_bar(stat = "identity", aes(fill = Sentiment))+
  scale_fill_manual("legend", values = c("Positive" = "green", "Neutral" = "black", "Negative" = "red"))+
  ggtitle("Barplot of Sentiment in Santa Barbara tweets")
```

```{r plot_sentiment_by_day}
# tally sentiment score per day
daily_sent <- tweets_sent %>%
  group_by(date) %>%
  summarize(sent_score = mean(sent_score, na.rm = T))

daily_sent %>%
  ggplot( aes(x = date, y = sent_score)) +
  geom_line() +
    labs(x = "Date",
    y = "Avg Sentiment Score",
    title = "Daily Tweet Sentiment",
    subtitle = "Thomas Fire Tweets")

```

Now let's try a new type of text visualization: the wordcloud.

```{r wordcloud}
words %>%
   anti_join(stop_words) %>%
   count(word) %>%
   with(wordcloud(word, n, max.words = 100))


```

```{r wordcloud_comp}

words %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

#### The quanteda package

quanteda is a package (actually a family of packages) full of tools for conducting text analysis. quanteda.sentiment (not yet on CRAN, download from github) is the quanteda modular package for conducting sentiment analysis.

quanteda has its own built in functions for cleaning text data. Let's take a look at some. First we have to clean the messy tweet data:

```{r create_corpus}
corpus <- corpus(data$Title) #enter quanteda
summary(corpus)
```

```{r quanteda_cleaning}
tokens <- tokens(corpus) #tokenize the text so each doc (page, in this case) is a list of tokens (words)

#examine the uncleaned version
tokens

#clean it up
tokens <- tokens(tokens, remove_punct = TRUE,
                      remove_numbers = TRUE)

tokens <- tokens_select(tokens, stopwords('english'),selection='remove') #stopwords lexicon built in to quanteda

#tokens <- tokens_wordstem(tokens) #stem words down to their base form for comparisons across tense and quantity
# since we are doing visual analysis we are leaving this out but if we were doing rigorous analysis we would want to do it.

tokens <- tokens_tolower(tokens)

```

We can use the kwic function (keywords-in-context) to briefly examine the context in which certain words or patterns appear.

```{r initial_analysis}
head(kwic(tokens, pattern = "climate", window = 3))

head(kwic(tokens, pattern = phrase("climate change"), window = 3))


```


```{r explore_hashtags}
hash_tweets <- tokens(corpus, remove_punct = TRUE) %>% 
               tokens_keep(pattern = "#*")

dfm_hash<- dfm(hash_tweets)

tstat_freq <- textstat_frequency(dfm_hash, n = 100)
head(tstat_freq, 10)

#tidytext gives us tools to convert to tidy from non-tidy formats
hash_tib<- tidy(dfm_hash)

hash_tib %>%
   count(term) %>%
   with(wordcloud(term, n, max.words = 100))


```


Create the sparse matrix representation known as the document-feature matrix. quanteda's textstat_polarity function has multiple ways to combine polarity to a single score. The sent_logit value to fun argument is the log of (pos/neg) counts.

```{r}

dfm <- dfm(tokens)

topfeatures(dfm, 12)

dfm.sentiment <- dfm_lookup(dfm, dictionary = data_dictionary_LSD2015)

head(textstat_polarity(tokens, data_dictionary_LSD2015, fun = sent_logit))


```

```{r cleaning_tweets}
tweets <- tibble(text = data$Title,
                  id = seq(1:length(data$Title)),
                 date = as.Date(data$Date,
                                '%m/%d/%y'),
                 sentiment = data$Sentiment,
                 emotion = data$Emotion) %>% 
  mutate(text = str_replace(string = text,
                            pattern = "http.*[:space:]",
                            replacement = ""),
         text = str_replace(string = text,
                            pattern = "http.*$",
                            replacement = ""),
         text = str_replace(string = text,
                            pattern = "@.*[:space:]",
                            replacement = ""),
         text = str_replace(string = text,
                            pattern = "@.*$",
                            replacement = ""),
         text = str_replace_all(string = text,
                               pattern = "rt",
                               replacement = ""),
         text = str_to_lower(text))

tweets$text <- iconv(tweets$text, 
                     "latin1", 
                     "ASCII", 
                     sub="")
```

```{r}
#tokenize tweets to individual words
words <- tweets %>%
  select(id, date, text) %>%
  unnest_tokens(output = word, input = text, token = "words") %>%
  anti_join(stop_words, by = "word") %>%
  left_join(bing_sent, by = "word") %>%
  left_join(
    tribble(
      ~sentiment, ~sent_score,
      "positive", 1,
      "negative", -1),
    by = "sentiment")

```

```{r}
ten_words <- words %>% 
  group_by(date, word) %>% 
  summarise(count = n()) %>% 
  group_by(date) %>% 
  slice_max(count, 
            n = 10,
            with_ties = FALSE) %>% 
  ggplot(aes(x = count,
             y = word)) +
  geom_col() +
  facet_wrap(~date, scales = "free") +
  guides(fill = "none") +
  labs(x = "Number of Times Word Occurs",
       y = "",
       title = "Top 10 Words per Day")


ten_words
```

### Assignment

You will use the tweet data from class today for each part of the following assignment.

1.  Think about how to further clean a twitter data set. Let's assume that the mentions of twitter accounts is not useful to us. Remove them from the text field of the tweets tibble.

2.  Compare the ten most common terms in the tweets per day.  Do you notice anything interesting?

3.  Adjust the wordcloud in the "wordcloud" chunk by coloring the positive and negative words so they are identifiable.

4. Let's say we are interested in the most prominent entities in the Twitter discussion.  Which are the top 10 most tagged accounts in the data set. Hint: the "explore_hashtags" chunk is a good starting point.

5. The Twitter data download comes with a variable called "Sentiment" that must be calculated by Brandwatch.  Use your own method to assign each tweet a polarity score (Positive, Negative, Neutral) and compare your classification to Brandwatch's (hint: you'll need to revisit the "raw_tweets" data frame).   

